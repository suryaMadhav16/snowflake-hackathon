# Documentation Website Crawler

A high-performance documentation crawler that extracts content from technical documentation websites, processes it using Groq for enhanced understanding, and organizes the content for effective retrieval and analysis.

## Features

- **Parallel Web Crawling**
  - Configurable concurrent crawling
  - Smart rate limiting
  - Resume functionality for interrupted crawls
  - Test mode for quick validation

- **Content Processing**
  - Markdown conversion with structural preservation
  - Automatic image downloading and organization
  - Code snippet extraction and analysis
  - Page summarization using Groq

- **Output Organization**
  - Structured data storage
  - Clean directory hierarchy
  - Image management with source tracking
  - Comprehensive metadata

## Prerequisites

- Python 3.9+
- Groq API key
- Internet connection
- Sufficient storage space

## Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/snowflake-hackathon.git
cd snowflake-hackathon
```

2. Create and activate a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Set up environment variables:
```bash
cp .env .env
# Edit .env with your configuration, especially the GROQ_API_KEY
```

## Usage

### Basic Crawling
```bash
python -m src.cli.main https://docs.example.com
```

### Test Mode (Limited to 5 pages)
```bash
python -m src.cli.main https://docs.example.com --test
```

### Advanced Options
```bash
python -m src.cli.main https://docs.example.com \
  --max-concurrent 10 \
  --output-dir ./custom_output \
  --debug
```

## Directory Structure

```
snowflake-hackathon/
├── data/
│   ├── raw/              # Raw markdown files
│   │   └── {domain}/
│   ├── processed/        # Processed JSON files
│   │   └── {domain}/
│   ├── images/           # Downloaded images
│   │   └── {domain}/
│   └── logs/            # Crawler logs
├── src/
│   ├── crawler/         # Core crawler logic
│   ├── models/          # Data models
│   ├── cli/             # Command line interface
│   └── config/          # Configuration management
└── tests/              # Test suite (coming soon)
```

## Configuration Options

| Environment Variable | Description | Default |
|---------------------|-------------|---------|
| GROQ_API_KEY | Your Groq API key | Required |
| CRAWLER_MAX_CONCURRENT | Maximum concurrent crawls | 5 |
| CRAWLER_REQUESTS_PER_SECOND | Rate limit | 2.0 |
| CRAWLER_DEBUG | Enable debug logging | False |

## Output Format

### Raw Data
- Markdown files with original content
- Downloaded images with source mapping
- Preserved structure and formatting

### Processed Data (JSON)
```json
{
  "metadata": {
    "url": "https://docs.example.com/page",
    "title": "Page Title",
    "crawled_at": "2024-01-13T12:00:00Z",
    "images": ["list", "of", "image", "paths"],
    "keywords": ["extracted", "keywords"]
  },
  "summary": "Page summary generated by Groq",
  "code_snippets": [
    {
      "language": "python",
      "code": "actual code",
      "description": "What this code does",
      "location": "Context in the page"
    }
  ]
}
```

## Error Handling

- Automatic retry for failed requests
- Comprehensive error logging
- Resume capability for interrupted crawls
- Rate limit respect
- Resource cleanup

## Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- crawl4ai library for web crawling capabilities
- Groq for advanced text processing
- Beautiful Soup for HTML processing
